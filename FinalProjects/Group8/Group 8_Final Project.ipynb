{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Importing dataframe as numeric without imputation#\n",
    "\n",
    "df = pd.read_csv(\"testdata.csv\", sep = \";\")\n",
    "\n",
    "df['Minimum Estimated Revenue'] = pd.to_numeric(df['Minimum Estimated Revenue'], errors='coerce')\n",
    "\n",
    "df['Last Funding Amount Currency'] = pd.to_numeric(df['Last Funding Amount Currency'], errors='coerce')\n",
    "\n",
    "df['Total Funding Amount Currency'] = pd.to_numeric(df['Total Funding Amount Currency'], errors='coerce')\n",
    "\n",
    "df['Number of Investors'] = pd.to_numeric(df['Number of Investors'], errors='coerce')\n",
    "\n",
    "df['Money Raised at IPO Currency'] = pd.to_numeric(df['Money Raised at IPO Currency'], errors='coerce')\n",
    "\n",
    "df['Valuation at IPO Currency'] = pd.to_numeric(df['Valuation at IPO Currency'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Mean Imputation#\n",
    "\n",
    "min_est_revmean = df['Minimum Estimated Revenue'].mean()\n",
    "df['Minimum Estimated Revenue'].fillna(min_est_revmean, inplace=True)\n",
    "\n",
    "lfacmean = df['Last Funding Amount Currency'].mean()\n",
    "df['Last Funding Amount Currency'].fillna(lfacmean, inplace=True)\n",
    "\n",
    "tfacmean = df['Total Funding Amount Currency'].mean()\n",
    "df['Total Funding Amount Currency'].fillna(tfacmean, inplace=True)\n",
    "\n",
    "noimean = df['Number of Investors'].mean()\n",
    "df['Number of Investors'].fillna(noimean, inplace=True)\n",
    "\n",
    "mricmean = df['Money Raised at IPO Currency'].mean()\n",
    "df['Money Raised at IPO Currency'].fillna(mricmean, inplace=True)\n",
    "\n",
    "vicmean = df['Valuation at IPO Currency'].mean()\n",
    "df['Valuation at IPO Currency'].fillna(vicmean, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import hmean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import numbers\n",
    "\n",
    "#KNN imputation, followed by deletion of rows under threshold of existing data, followed by global mean imputation#\n",
    "#This method provides the highest accuracy for Random Forest Regressor#\n",
    "\n",
    "def weighted_hamming(data):\n",
    "    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n",
    "        the values between point A and point B are different, else it is equal the relative frequency of the\n",
    "        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n",
    "        up to a constant factor.\n",
    "\n",
    "        @params:\n",
    "            - data = a pandas data frame of categorical variables\n",
    "\n",
    "        @returns:\n",
    "            - distance_matrix = a distance matrix with pairwise distance for all attributes\n",
    "    \"\"\"\n",
    "    categories_dist = []\n",
    "    \n",
    "    for category in data:\n",
    "        X = pd.get_dummies(data[category])\n",
    "        X_mean = X * X.mean()\n",
    "        X_dot = X_mean.dot(X.transpose())\n",
    "        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n",
    "        categories_dist.append(X_np)\n",
    "    categories_dist = np.array(categories_dist)\n",
    "    distances = hmean(categories_dist, axis=0)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n",
    "    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n",
    "        - Continuous\n",
    "        - Categorical\n",
    "        For ordinal values, provide a numerical representation taking the order into account.\n",
    "        Categorical variables are transformed into a set of binary ones.\n",
    "        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n",
    "        variables are all normalized in the process.\n",
    "        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n",
    "        \n",
    "        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n",
    "        like other distance metrics provided by scipy.\n",
    "\n",
    "        @params:\n",
    "            - data                  = pandas dataframe to compute distances on.\n",
    "            - numeric_distances     = the metric to apply to continuous attributes.\n",
    "                                      \"euclidean\" and \"cityblock\" available.\n",
    "                                      Default = \"euclidean\"\n",
    "            - categorical_distances = the metric to apply to binary attributes.\n",
    "                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                      available. Default = \"jaccard\"\n",
    "\n",
    "        @returns:\n",
    "            - the distance matrix\n",
    "    \"\"\"\n",
    "    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n",
    "    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n",
    "    number_of_variables = data.shape[1]\n",
    "    number_of_observations = data.shape[0]\n",
    "\n",
    "    # Get the type of each attribute (Numeric or categorical)\n",
    "    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n",
    "    is_all_numeric = sum(is_numeric) == len(is_numeric)\n",
    "    is_all_categorical = sum(is_numeric) == 0\n",
    "    is_mixed_type = not is_all_categorical and not is_all_numeric\n",
    "\n",
    "    # Check the content of the distances parameter\n",
    "    if numeric_distance not in possible_continuous_distances:\n",
    "        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n",
    "        return None\n",
    "    elif categorical_distance not in possible_binary_distances:\n",
    "        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n",
    "        return None\n",
    "\n",
    "    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n",
    "    if is_mixed_type:\n",
    "        number_of_numeric_var = sum(is_numeric)\n",
    "        number_of_categorical_var = number_of_variables - number_of_numeric_var\n",
    "        data_numeric = data.iloc[:, is_numeric]\n",
    "        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n",
    "        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n",
    "\n",
    "    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n",
    "    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "    # but the value are properly replaced\n",
    "    if is_mixed_type:\n",
    "        data_numeric.fillna(data_numeric.mean(), inplace=True)\n",
    "        for x in data_categorical:\n",
    "            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n",
    "    elif is_all_numeric:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "    else:\n",
    "        for x in data:\n",
    "            data[x].fillna(data[x].mode()[0], inplace=True)\n",
    "\n",
    "    # \"Dummifies\" categorical variables in place\n",
    "    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.get_dummies(data_categorical)\n",
    "        else:\n",
    "            data = pd.get_dummies(data)\n",
    "    elif not is_all_numeric and categorical_distance == 'hamming':\n",
    "        if is_mixed_type:\n",
    "            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n",
    "        else:\n",
    "            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n",
    "\n",
    "    if is_all_numeric:\n",
    "        result_matrix = cdist(data, data, metric=numeric_distance)\n",
    "    elif is_all_categorical:\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_matrix = weighted_hamming(data)\n",
    "        else:\n",
    "            result_matrix = cdist(data, data, metric=categorical_distance)\n",
    "    else:\n",
    "        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n",
    "        if categorical_distance == \"weighted-hamming\":\n",
    "            result_categorical = weighted_hamming(data_categorical)\n",
    "        else:\n",
    "            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n",
    "        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n",
    "                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n",
    "\n",
    "    # Fill the diagonal with NaN values\n",
    "    np.fill_diagonal(result_matrix, np.nan)\n",
    "\n",
    "    return pd.DataFrame(result_matrix)\n",
    "\n",
    "\n",
    "def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n",
    "               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n",
    "    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n",
    "        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n",
    "        remains missing. If there is a problem in the parameters provided, returns None.\n",
    "        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n",
    "\n",
    "        @params:\n",
    "            - target                        = a vector of n values with missing values that you want to impute. The length has\n",
    "                                              to be at least n = 3.\n",
    "            - attributes                    = a data frame of attributes with n rows to match the target variable\n",
    "            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n",
    "                                              value between 1 and n.\n",
    "            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n",
    "                                              Default = \"mean\"\n",
    "            - numeric_distances             = the metric to apply to continuous attributes.\n",
    "                                              \"euclidean\" and \"cityblock\" available.\n",
    "                                              Default = \"euclidean\"\n",
    "            - categorical_distances         = the metric to apply to binary attributes.\n",
    "                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n",
    "                                              available. Default = \"jaccard\"\n",
    "            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n",
    "                                              the correct value. Default = 0.5\n",
    "\n",
    "        @returns:\n",
    "            target_completed        = the vector of target values with missing value replaced. If there is a problem\n",
    "                                      in the parameters, return None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get useful variables\n",
    "    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n",
    "    number_observations = len(target)\n",
    "    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n",
    "\n",
    "    # Check for possible errors\n",
    "    if number_observations < 3:\n",
    "        print(\"Not enough observations.\")\n",
    "        return None\n",
    "    if attributes.shape[0] != number_observations:\n",
    "        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n",
    "        return None\n",
    "    if k_neighbors > number_observations or k_neighbors < 1:\n",
    "        print(\"The range of the number of neighbors is incorrect.\")\n",
    "        return None\n",
    "    if aggregation_method not in possible_aggregation_method:\n",
    "        print(\"The aggregation method is incorrect.\")\n",
    "        return None\n",
    "    if not is_target_numeric and aggregation_method != \"mode\":\n",
    "        print(\"The only method allowed for categorical target variable is the mode.\")\n",
    "        return None\n",
    "\n",
    "    # Make sure the data are in the right format\n",
    "    target = pd.DataFrame(target)\n",
    "    attributes = pd.DataFrame(attributes)\n",
    "\n",
    "    # Get the distance matrix and check whether no error was triggered when computing it\n",
    "    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n",
    "    if distances is None:\n",
    "        return None\n",
    "\n",
    "    # Get the closest points and compute the correct aggregation method\n",
    "    for i, value in enumerate(target.iloc[:, 0]):\n",
    "        if pd.isnull(value):\n",
    "            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n",
    "            closest_to_target = target.iloc[order, :]\n",
    "            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n",
    "            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n",
    "            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n",
    "                continue\n",
    "            elif aggregation_method == \"mean\":\n",
    "                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            elif aggregation_method == \"median\":\n",
    "                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n",
    "            else:\n",
    "                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n",
    "\n",
    "    return target\n",
    "\n",
    "knn_impute(df['Minimum Estimated Revenue'],df,5,missing_neighbors_threshold=0.9)\n",
    "\n",
    "df = df.dropna(thresh = 6)\n",
    "\n",
    "min_est_revmean = df['Minimum Estimated Revenue'].mean()\n",
    "df['Minimum Estimated Revenue'].fillna(min_est_revmean, inplace=True)\n",
    "\n",
    "lfacmean = df['Last Funding Amount Currency'].mean()\n",
    "df['Last Funding Amount Currency'].fillna(lfacmean, inplace=True)\n",
    "\n",
    "tfacmean = df['Total Funding Amount Currency'].mean()\n",
    "df['Total Funding Amount Currency'].fillna(tfacmean, inplace=True)\n",
    "\n",
    "noimean = df['Number of Investors'].mean()\n",
    "df['Number of Investors'].fillna(noimean, inplace=True)\n",
    "\n",
    "mricmean = df['Money Raised at IPO Currency'].mean()\n",
    "df['Money Raised at IPO Currency'].fillna(mricmean, inplace=True)\n",
    "\n",
    "vicmean = df['Valuation at IPO Currency'].mean()\n",
    "df['Valuation at IPO Currency'].fillna(vicmean, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Global Mode Imputation for Missing Values#\n",
    "\n",
    "df = pd.read_csv(\"testdata.csv\", sep = \";\")\n",
    "\n",
    "df['Minimum Estimated Revenue'] = pd.to_numeric(df['Minimum Estimated Revenue'], errors='coerce')\n",
    "min_est_revmean = df['Minimum Estimated Revenue'].mode()\n",
    "df['Minimum Estimated Revenue'].fillna(min_est_revmean, inplace=True)\n",
    "\n",
    "df['Last Funding Amount Currency'] = pd.to_numeric(df['Last Funding Amount Currency'], errors='coerce')\n",
    "lfacmean = df['Last Funding Amount Currency'].mode()\n",
    "df['Last Funding Amount Currency'].fillna(lfacmean, inplace=True)\n",
    "\n",
    "df['Total Funding Amount Currency'] = pd.to_numeric(df['Total Funding Amount Currency'], errors='coerce')\n",
    "tfacmean = df['Total Funding Amount Currency'].mode()\n",
    "df['Total Funding Amount Currency'].fillna(tfacmean, inplace=True)\n",
    "\n",
    "df['Number of Investors'] = pd.to_numeric(df['Number of Investors'], errors='coerce')\n",
    "noimean = df['Number of Investors'].mode()\n",
    "df['Number of Investors'].fillna(noimean, inplace=True)\n",
    "\n",
    "df['Money Raised at IPO Currency'] = pd.to_numeric(df['Money Raised at IPO Currency'], errors='coerce')\n",
    "mricmean = df['Money Raised at IPO Currency'].mode()\n",
    "df['Money Raised at IPO Currency'].fillna(mricmean, inplace=True)\n",
    "\n",
    "df['Valuation at IPO Currency'] = pd.to_numeric(df['Valuation at IPO Currency'], errors='coerce')\n",
    "vicmean = df['Valuation at IPO Currency'].mode()\n",
    "df['Valuation at IPO Currency'].fillna(vicmean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Global Median Imputation for Missing Values#\n",
    "\n",
    "df = pd.read_csv(\"testdata.csv\", sep = \";\")\n",
    "\n",
    "df['Minimum Estimated Revenue'] = pd.to_numeric(df['Minimum Estimated Revenue'], errors='coerce')\n",
    "min_est_revmean = df['Minimum Estimated Revenue'].median()\n",
    "df['Minimum Estimated Revenue'].fillna(min_est_revmean, inplace=True)\n",
    "\n",
    "df['Last Funding Amount Currency'] = pd.to_numeric(df['Last Funding Amount Currency'], errors='coerce')\n",
    "lfacmean = df['Last Funding Amount Currency'].median()\n",
    "df['Last Funding Amount Currency'].fillna(lfacmean, inplace=True)\n",
    "\n",
    "df['Total Funding Amount Currency'] = pd.to_numeric(df['Total Funding Amount Currency'], errors='coerce')\n",
    "tfacmean = df['Total Funding Amount Currency'].median()\n",
    "df['Total Funding Amount Currency'].fillna(tfacmean, inplace=True)\n",
    "\n",
    "df['Number of Investors'] = pd.to_numeric(df['Number of Investors'], errors='coerce')\n",
    "noimean = df['Number of Investors'].median()\n",
    "df['Number of Investors'].fillna(noimean, inplace=True)\n",
    "\n",
    "df['Money Raised at IPO Currency'] = pd.to_numeric(df['Money Raised at IPO Currency'], errors='coerce')\n",
    "mricmean = df['Money Raised at IPO Currency'].median()\n",
    "df['Money Raised at IPO Currency'].fillna(mricmean, inplace=True)\n",
    "\n",
    "df['Valuation at IPO Currency'] = pd.to_numeric(df['Valuation at IPO Currency'],errors='coerce')\n",
    "vicmean = df['Valuation at IPO Currency'].median()\n",
    "df['Valuation at IPO Currency'].fillna(vicmean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Linear Regression Model and Plot#\n",
    "\n",
    "df_x = df.iloc[:,1:8]\n",
    "df_y = df.iloc[:,8:]\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size = 0.2, random_state = 4)\n",
    "reg.fit(x_train, y_train)\n",
    "model_predictions = reg.predict(x_test)\n",
    "plt.scatter(x_test.index,y_test, color='black')\n",
    "plt.scatter(x_test.index,model_predictions, color='blue')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression Accuracy Measures#\n",
    "\n",
    "errors = abs(model_predictions - y_test)\n",
    "sway_metric = errors/1000\n",
    "mean_sway = sway_metric.mean()\n",
    "print('R^2 is:', r2_score(y_test,model_predictions))\n",
    "print('Mean of residual errors is:', errors.mean())\n",
    "print('Mean sway percentage is:', mean_sway)\n",
    "print('Standard deviation of errors is:', errors.std())\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', reg.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, model_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Random Forest Regressor Model\n",
    "\n",
    "df_x = df.iloc[:,1:8]\n",
    "df_y = df.iloc[:,8:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size = 0.2, random_state = 4)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 5500, random_state = 42)\n",
    "\n",
    "rf.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "randf_predictions = rf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Accuracy Measures#\n",
    "\n",
    "accuracymeas = rf.score(x_test,y_test)\n",
    "accuracymeas2 = mean_squared_error(y_test, randf_predictions)\n",
    "print('R^2 score is:',accuracymeas)\n",
    "print('Mean squared error is:',accuracymeas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importances dataframe and barplot of importances in ascending order#\n",
    "\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = x_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances.plot(kind='barh')\n",
    "feature_importances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
